@article{Bekiroglu2017,
 abstract = {While Industrial robots are very successful in many areas of industrial manufacturing, assembly automation still suffers from complex time consuming programming and the need of dedicated hardware. ABB has developed YuMi, a collaborative inherently safe assembly robot that is expected to reduce integration costs significantly by offering a standardized hardware setup and simple fitting of the robot into existing workplaces. Internal Pilot testing at ABB has however shown that when YuMi is programmed with traditional methods the programming time even for simple assembly tasks will remain very long.
The SARAFun project has been formed to enable a non-expert user to integrate a new bi-manual assembly task on a YuMi robot in less than a day. This will be accomplished by augmenting the YuMi robot with cutting edge sensory and cognitive abilities as well as reasoning abilities required to plan and execute an assembly task.
The overall conceptual approach is that the robot should be capable of learning and executing assembly tasks in a human like manner. Studies will be made to understand how human assembly workers learn and perform assembly tasks. The human performance will be modelled and transferred to the YuMi robot as assembly skills. The robot will learn assembly tasks, such as insertion or folding, by observing the task being performed by a human instructor. The robot will then analyze the task and generate an assembly program, including exception handling, and design 3D printable fingers tailored for gripping the parts at hand. Aided by the human instructor, the robot will finally learn to perform the actual assembly task, relying on sensory feedback from vision, force and tactile sensing as well as physical human robot interaction. During this phase the robot will gradually improve its understanding of the assembly at hand until it is capable of performing the assembly in a fast and robust manner.},
 author = {Bekiroglu, Y. and Haschke, R. and Karayiannidis, Y. and Mariolis, I. and McIntyre, J. and Malec, J. and Remazeilles, A.},
 doi = {doi:10.21820/23987073.2017.5.67},
 journal = {Impact},
 keywords = {Automatic finger generation, Robotics for manufacturing, Human robot interaction, Robotic control, Knowledge integration, Robotic sensors, Teaching by demonstration, Industrial, Industry, Vision-based and tactile sensing for learning object manipulation, H2020, Robotics, Assembly, Industrial, Industry, H2020, Robotic Assembly, Teaching by demonstration, Learning by doing, Automatic finger generation, Human robot interaction, Knowledge integration, Robotic sensors, Learning, development and adaptation, Robotics for manufacturing, Robotic control, Bi-manual robot, Assembly of two tasks, Learning by demonstration, Automated 3D finger generation, Vision-based and tactile sensing for learning object manipulation, Customized grasping, development and adaptation, Automated 3D finger generation, Robotic Assembly, Customized grasping, Bi-manual robot, Assembly of two tasks, Learning by doing, Assembly, Learning by demonstration, Learning},
 number = {5},
 pages = {67-69},
 title = {SARAFun, Smart Assembly Robot with Advanced FUNctionalities, H2020},
 url = {https://www.ingentaconnect.com/content/sil/impact/2017/00002017/00000005/art00024},
 volume = {2017},
 year = {2017}
}

